{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZh3aklt3ZET"
   },
   "source": [
    "### Tiny NeRF + StableDiffusion + Score Distillation Sampling (Dreamfusion)\n",
    "\n",
    "TinyNeRF Based on: https://colab.research.google.com/drive/1rO8xo0TemN67d4mTpakrKrLp03b9bgCX#scrollTo=ptTYjWao3VsM\n",
    "from https://github.com/krrish94/nerf-pytorch, which reimplements https://www.matthewtancik.com/nerf\n",
    "\n",
    "Stable Diffusion from https://github.com/CompVis/stable-diffusion,\n",
    "using parts of scripts/txt2img.py and ldm/models/diffusion/ddim.py\n",
    "\n",
    "To run/install has the same requirements as the CompVis stable-diffusion repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ptTYjWao3VsM"
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from imageio import imsave\n",
    "\n",
    "# Imports for stablediffusion txt2img\n",
    "\n",
    "import argparse, os, sys, glob\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, trange\n",
    "from itertools import islice\n",
    "from einops import rearrange\n",
    "import time\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch import autocast\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monkeypatch the ldm ddpm.py LatentDiffusion class instance\n",
    "# to remove torch.no_grad decorator\n",
    "def encode_first_stage(self, x):\n",
    "    if hasattr(self, \"split_input_params\"):\n",
    "        if self.split_input_params[\"patch_distributed_vq\"]:\n",
    "            ks = self.split_input_params[\"ks\"]  # eg. (128, 128)                                                                                                                     \n",
    "            stride = self.split_input_params[\"stride\"]  # eg. (64, 64)                                                                                                               \n",
    "            df = self.split_input_params[\"vqf\"]\n",
    "            self.split_input_params['original_image_size'] = x.shape[-2:]\n",
    "            bs, nc, h, w = x.shape\n",
    "            if ks[0] > h or ks[1] > w:\n",
    "                ks = (min(ks[0], h), min(ks[1], w))\n",
    "                print(\"reducing Kernel\")\n",
    "\n",
    "            if stride[0] > h or stride[1] > w:\n",
    "                stride = (min(stride[0], h), min(stride[1], w))\n",
    "                print(\"reducing stride\")\n",
    "\n",
    "            fold, unfold, normalization, weighting = self.get_fold_unfold(x, ks, stride, df=df)\n",
    "            z = unfold(x)  # (bn, nc * prod(**ks), L)                                                                                                                                \n",
    "            z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )                                                                                    \n",
    "\n",
    "            output_list = [self.first_stage_model.encode(z[:, :, :, :, i])\n",
    "                           for i in range(z.shape[-1])]\n",
    "\n",
    "            o = torch.stack(output_list, axis=-1)\n",
    "            o = o * weighting\n",
    "\n",
    "            # Reverse reshape to img shape                                                                                                                                           \n",
    "            o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)                                                                                                 \n",
    "            # stitch crops together                                                                                                                                                  \n",
    "            decoded = fold(o)\n",
    "            decoded = decoded / normalization\n",
    "            return decoded\n",
    "\n",
    "        else:\n",
    "            return self.first_stage_model.encode(x)\n",
    "    else:\n",
    "        return self.first_stage_model.encode(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_config(config, ckpt, verbose=False):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    if \"global_step\" in pl_sd:\n",
    "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    if len(m) > 0 and verbose:\n",
    "        print(\"missing keys:\")\n",
    "        print(m)\n",
    "    if len(u) > 0 and verbose:\n",
    "        print(\"unexpected keys:\")\n",
    "        print(u)\n",
    "\n",
    "    model.encode_first_stage = encode_first_stage.__get__(model, type(model))\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize SD model (txt2img.py)\n",
    "\n",
    "seed_everything(123)\n",
    "\n",
    "config = OmegaConf.load(\"configs/stable-diffusion/v1-inference.yaml\")\n",
    "model = load_model_from_config(config, \"sd-v1-4.ckpt\")\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.model = model.model.to(torch.float16)\n",
    "sd_model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = DDIMSampler(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generate DDIM schedule to provide alpha values at each timestep\n",
    "ddim_steps = 50\n",
    "ddim_eta = 0.0\n",
    "sampler.make_schedule(ddim_num_steps=ddim_steps, ddim_eta=ddim_eta, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qbdf8GDL4_0Z"
   },
   "outputs": [],
   "source": [
    "# Code from pytorch-nerf tinyNeRF \n",
    "\n",
    "def meshgrid_xy(tensor1: torch.Tensor, tensor2: torch.Tensor) -> (torch.Tensor, torch.Tensor):\n",
    "    \"\"\"Mimick np.meshgrid(..., indexing=\"xy\") in pytorch. torch.meshgrid only allows \"ij\" indexing.\n",
    "    (If you're unsure what this means, safely skip trying to understand this, and run a tiny example!)\n",
    "\n",
    "    Args:\n",
    "      tensor1 (torch.Tensor): Tensor whose elements define the first dimension of the returned meshgrid.\n",
    "      tensor2 (torch.Tensor): Tensor whose elements define the second dimension of the returned meshgrid.\n",
    "    \"\"\"\n",
    "    ii, jj = torch.meshgrid(tensor1, tensor2)\n",
    "    return ii.transpose(-1, -2), jj.transpose(-1, -2)\n",
    "\n",
    "\n",
    "def cumprod_exclusive(tensor: torch.Tensor) -> torch.Tensor:\n",
    "  r\"\"\"Mimic functionality of tf.math.cumprod(..., exclusive=True), as it isn't available in PyTorch.\n",
    "\n",
    "  Args:\n",
    "    tensor (torch.Tensor): Tensor whose cumprod (cumulative product, see `torch.cumprod`) along dim=-1\n",
    "      is to be computed.\n",
    "  \n",
    "  Returns:\n",
    "    cumprod (torch.Tensor): cumprod of Tensor along dim=-1, mimiciking the functionality of\n",
    "      tf.math.cumprod(..., exclusive=True) (see `tf.math.cumprod` for details).\n",
    "  \"\"\"\n",
    "  # TESTED\n",
    "  # Only works for the last dimension (dim=-1)\n",
    "  dim = -1\n",
    "  # Compute regular cumprod first (this is equivalent to `tf.math.cumprod(..., exclusive=False)`).\n",
    "  cumprod = torch.cumprod(tensor, dim)\n",
    "  # \"Roll\" the elements along dimension 'dim' by 1 element.\n",
    "  cumprod = torch.roll(cumprod, 1, dim)\n",
    "  # Replace the first element by \"1\" as this is what tf.cumprod(..., exclusive=True) does.\n",
    "  cumprod[..., 0] = 1.\n",
    "  \n",
    "  return cumprod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcCuOfj-7OPN"
   },
   "source": [
    "#### Compute the \"bundle\" of rays through all pixels of an image (tinyNeRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EHNwlsOT7NTp"
   },
   "outputs": [],
   "source": [
    "# Code from pytorch-nerf tinyNeRF\n",
    "\n",
    "\n",
    "def get_ray_bundle(height: int, width: int, focal_length: float, tform_cam2world: torch.Tensor):\n",
    "  r\"\"\"Compute the bundle of rays passing through all pixels of an image (one ray per pixel).\n",
    "\n",
    "  Args:\n",
    "    height (int): Height of an image (number of pixels).\n",
    "    width (int): Width of an image (number of pixels).\n",
    "    focal_length (float or torch.Tensor): Focal length (number of pixels, i.e., calibrated intrinsics).\n",
    "    tform_cam2world (torch.Tensor): A 6-DoF rigid-body transform (shape: :math:`(4, 4)`) that\n",
    "      transforms a 3D point from the camera frame to the \"world\" frame for the current example.\n",
    "  \n",
    "  Returns:\n",
    "    ray_origins (torch.Tensor): A tensor of shape :math:`(width, height, 3)` denoting the centers of\n",
    "      each ray. `ray_origins[i][j]` denotes the origin of the ray passing through pixel at\n",
    "      row index `j` and column index `i`.\n",
    "      (TODO: double check if explanation of row and col indices convention is right).\n",
    "    ray_directions (torch.Tensor): A tensor of shape :math:`(width, height, 3)` denoting the\n",
    "      direction of each ray (a unit vector). `ray_directions[i][j]` denotes the direction of the ray\n",
    "      passing through the pixel at row index `j` and column index `i`.\n",
    "      (TODO: double check if explanation of row and col indices convention is right).\n",
    "  \"\"\"  \n",
    "  ii, jj = meshgrid_xy(\n",
    "      torch.arange(width).to(tform_cam2world),\n",
    "      torch.arange(height).to(tform_cam2world)\n",
    "  )\n",
    "  directions = torch.stack([(ii - width * .5) / focal_length,\n",
    "                            -(jj - height * .5) / focal_length,\n",
    "                            -torch.ones_like(ii)\n",
    "                           ], dim=-1)\n",
    "  ray_directions = torch.sum(directions[..., None, :] * tform_cam2world[:3, :3], dim=-1)\n",
    "  ray_origins = tform_cam2world[:3, -1].expand(ray_directions.shape)\n",
    "  return ray_origins, ray_directions\n",
    "\n",
    "# Note that this grid of rays are not normalized (doesn't matter for our purposes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1bcBtvR-s0_"
   },
   "source": [
    "#### Compute \"query\" 3D points given the \"bundle\" of rays (tinyNeRF)\n",
    "\n",
    "We assume that a _near_ and a _far_ clipping distance are provided that delineate the volume of interest. Each ray is evaluated only within these bounds. We randomly sample points along each ray, while trying to ensure most parts of the ray's trajectory are spanned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wAAfDK2L-faR"
   },
   "outputs": [],
   "source": [
    "# Code from pytorch-nerf tinyNeRF\n",
    "\n",
    "def compute_query_points_from_rays(\n",
    "    ray_origins: torch.Tensor,\n",
    "    ray_directions: torch.Tensor,\n",
    "    near_thresh: float,\n",
    "    far_thresh: float,\n",
    "    num_samples: int,\n",
    "    randomize: Optional[bool] = True\n",
    ") -> (torch.Tensor, torch.Tensor):\n",
    "  r\"\"\"Compute query 3D points given the \"bundle\" of rays. The near_thresh and far_thresh\n",
    "  variables indicate the bounds within which 3D points are to be sampled.\n",
    "\n",
    "  Args:\n",
    "    ray_origins (torch.Tensor): Origin of each ray in the \"bundle\" as returned by the\n",
    "      `get_ray_bundle()` method (shape: :math:`(width, height, 3)`).\n",
    "    ray_directions (torch.Tensor): Direction of each ray in the \"bundle\" as returned by the\n",
    "      `get_ray_bundle()` method (shape: :math:`(width, height, 3)`).\n",
    "    near_thresh (float): The 'near' extent of the bounding volume (i.e., the nearest depth\n",
    "      coordinate that is of interest/relevance).\n",
    "    far_thresh (float): The 'far' extent of the bounding volume (i.e., the farthest depth\n",
    "      coordinate that is of interest/relevance).\n",
    "    num_samples (int): Number of samples to be drawn along each ray. Samples are drawn\n",
    "      randomly, whilst trying to ensure \"some form of\" uniform spacing among them.\n",
    "    randomize (optional, bool): Whether or not to randomize the sampling of query points.\n",
    "      By default, this is set to `True`. If disabled (by setting to `False`), we sample\n",
    "      uniformly spaced points along each ray in the \"bundle\".\n",
    "  \n",
    "  Returns:\n",
    "    query_points (torch.Tensor): Query points along each ray\n",
    "      (shape: :math:`(width, height, num_samples, 3)`).\n",
    "    depth_values (torch.Tensor): Sampled depth values along each ray\n",
    "      (shape: :math:`(num_samples)`).\n",
    "  \"\"\"\n",
    "  # TESTED\n",
    "  # shape: (num_samples)\n",
    "  depth_values = torch.linspace(near_thresh, far_thresh, num_samples).to(ray_origins)\n",
    "  if randomize is True:\n",
    "    # ray_origins: (width, height, 3)\n",
    "    # noise_shape = (width, height, num_samples)\n",
    "    noise_shape = list(ray_origins.shape[:-1]) + [num_samples]\n",
    "    # depth_values: (num_samples)\n",
    "    depth_values = depth_values \\\n",
    "        + torch.rand(noise_shape).to(ray_origins) * (far_thresh\n",
    "            - near_thresh) / num_samples\n",
    "  # (width, height, num_samples, 3) = (width, height, 1, 3) + (width, height, 1, 3) * (num_samples, 1)\n",
    "  # query_points:  (width, height, num_samples, 3)\n",
    "  query_points = ray_origins[..., None, :] + ray_directions[..., None, :] * depth_values[..., :, None]\n",
    "  return query_points, depth_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqcXnLXpDgsR"
   },
   "source": [
    "#### Volumetric rendering (TinyNeRF)\n",
    "> **NOTE**: This volumetric rendering module (like the authors' tiny_nerf [Colab notebook](https://colab.research.google.com/github/bmild/nerf/blob/master/tiny_nerf.ipynb) does not implement 5D input (which includes view directions, in addition to X, Y, Z coordinates). It also does not implement the hierarchical sampling procedure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is modified to include a (simple) illumination model,\n",
    "### using surface normals extracted from the gradient of the NeRF density field\n",
    "### This is probably very inefficient - need to backprop through these normals\n",
    "### Must be a better way to do this - probably finite differences\n",
    "\n",
    "def render_volume_density(\n",
    "    radiance_field: torch.Tensor,\n",
    "    ray_origins: torch.Tensor,\n",
    "    depth_values: torch.Tensor,\n",
    "    bg_color: torch.Tensor,\n",
    ") -> (torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor):\n",
    "  r\"\"\"Differentiably renders a radiance field, given the origin of each ray in the\n",
    "  \"bundle\", and the sampled depth values along them.\n",
    "\n",
    "  Args:\n",
    "    radiance_field (torch.Tensor): A \"field\" where, at each query location (X, Y, Z),\n",
    "      we have an emitted (RGB) color and a volume density (denoted :math:`\\sigma` in\n",
    "      the paper) (shape: :math:`(width, height, num_samples, 4)`).\n",
    "    ray_origins (torch.Tensor): Origin of each ray in the \"bundle\" as returned by the\n",
    "      `get_ray_bundle()` method (shape: :math:`(width, height, 3)`).\n",
    "    depth_values (torch.Tensor): Sampled depth values along each ray\n",
    "      (shape: :math:`(num_samples)`).\n",
    "  \n",
    "  Returns:\n",
    "    rgb_map (torch.Tensor): Rendered RGB image (shape: :math:`(width, height, 3)`).\n",
    "    depth_map (torch.Tensor): Rendered depth image (shape: :math:`(width, height)`).\n",
    "    grad_map (torch.Tensor): Accumulated surfae \n",
    "    norm_reg_map (torch.tensor):\n",
    "    T_map (torch.Tensor): # TODO: Double-check (I think this is the accumulated\n",
    "      transmittance map).\n",
    "  \"\"\"\n",
    "  # Use a RELU activation function for density field (rather than exp in paper).\n",
    "  # Not smooth like in paper - but on other hand, easier to make vanish (exp(-4.5) = 1%)\n",
    "  # We really want to encourage the model to clear space\n",
    "\n",
    "  sigma_a = torch.nn.functional.relu(radiance_field[..., 3])\n",
    "  # Swap between NeRF colors (obj_col_w=0) and a constant col obj_col (obj_col_w=1)\n",
    "  rgb = torch.sigmoid(radiance_field[..., :3])\n",
    "\n",
    "  # The original tinyNerf used a large depth value for the final point in the ray,\n",
    "  # Here we set this to zero, ignoring this point\n",
    "\n",
    "  small = torch.tensor([0e-6], dtype=ray_origins.dtype, device=ray_origins.device)\n",
    "  dists = torch.cat((depth_values[..., 1:] - depth_values[..., :-1],\n",
    "                  small.expand(depth_values[..., :1].shape)), dim=-1)\n",
    "\n",
    "  # Integrated opacity for each ray segment. Note that this uses samples from the density field\n",
    "  # at the front of each ray segment - think it would reduce noise a little bit if we averaged the\n",
    "  # front and back\n",
    "  alpha = 1. - torch.exp(-sigma_a * dists)\n",
    "  # Weights for each ray segment -> alpha compositing. Opaque segments reduce visibility\n",
    "  # of those deeper in the volume\n",
    "  weights = alpha * cumprod_exclusive(1. - alpha + 1e-10)\n",
    "  # Transparency map - used for bg compositing and to estimate occupancy of nerf volume\n",
    "  T_map = weights.sum(dim=-1)\n",
    "  \n",
    "  rgb_map = (weights[ ... , None] * rgb).sum(dim=-2) + (1-T_map[..., None])*bg_color[None, None, :]\n",
    "  \n",
    "  depth_map = (weights * depth_values).sum(dim=-1)\n",
    "\n",
    "  return rgb_map, depth_map, T_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i94CgahGHBEV"
   },
   "source": [
    "#### Positional encoding (TinyNeRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rrbs7YoMHAbF"
   },
   "outputs": [],
   "source": [
    "# pytorch-tinyNeRF code\n",
    "def positional_encoding(\n",
    "    tensor, num_encoding_functions=6, include_input=True, log_sampling=True\n",
    ") -> torch.Tensor:\n",
    "  r\"\"\"Apply positional encoding to the input.\n",
    "\n",
    "  Args:\n",
    "    tensor (torch.Tensor): Input tensor to be positionally encoded.\n",
    "    num_encoding_functions (optional, int): Number of encoding functions used to\n",
    "        compute a positional encoding (default: 6).\n",
    "    include_input (optional, bool): Whether or not to include the input in the\n",
    "        computed positional encoding (default: True).\n",
    "    log_sampling (optional, bool): Sample logarithmically in frequency space, as\n",
    "        opposed to linearly (default: True).\n",
    "  \n",
    "  Returns:\n",
    "    (torch.Tensor): Positional encoding of the input tensor.\n",
    "  \"\"\"\n",
    "  # The input tensor is added to the positional encoding\n",
    "  # Not optional for this notebook as we need the real position to\n",
    "  # add the initial \"gaussian\" bump\n",
    "  encoding = [tensor] if include_input else []\n",
    "  # Now, encode the input using a set of high-frequency functions and append the\n",
    "  # resulting values to the encoding.\n",
    "  frequency_bands = None\n",
    "  if log_sampling:\n",
    "      frequency_bands = 2.0 ** torch.linspace(\n",
    "            0.0,\n",
    "            num_encoding_functions - 1,\n",
    "            num_encoding_functions,\n",
    "            dtype=tensor.dtype,\n",
    "            device=tensor.device,\n",
    "        )\n",
    "  else:\n",
    "      frequency_bands = torch.linspace(\n",
    "          2.0 ** 0.0,\n",
    "          2.0 ** (num_encoding_functions - 1),\n",
    "          num_encoding_functions,\n",
    "          dtype=tensor.dtype,\n",
    "          device=tensor.device,\n",
    "      )\n",
    "\n",
    "  for freq in frequency_bands:\n",
    "      for func in [torch.sin, torch.cos]:\n",
    "          encoding.append(func(tensor * freq))\n",
    "\n",
    "  # Special case, for no positional encoding\n",
    "  if len(encoding) == 1:\n",
    "      return encoding[0]\n",
    "  else:\n",
    "      return torch.cat(encoding, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgoNR03iIs7R"
   },
   "source": [
    "## TinyNeRF: Network architecture (pytorch-tinyNeRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UjFN6FNzIqxl"
   },
   "outputs": [],
   "source": [
    "# Added an extra layer. Also added the \"seed\" gaussian as a constant output,\n",
    "# Model made to omit zero density outside a bounded sphere - mipNeRF 360\n",
    "# has a much cleverer solution with space rescaling.\n",
    "# Without this hard to minimize haze away from object.\n",
    "\n",
    "r2_max = 1**2\n",
    "\n",
    "class VeryTinyNerfModel(torch.nn.Module):\n",
    "  r\"\"\"Define a \"very tiny\" NeRF model comprising three fully connected layers.\n",
    "  \"\"\"\n",
    "  def __init__(self, filter_size=128, num_encoding_functions=6, l_s=5, s_s=0.2):\n",
    "    super(VeryTinyNerfModel, self).__init__()\n",
    "    self.l_s = l_s # Amplitude of initial gaussian bump (centred at origin)\n",
    "    self.s_s = s_s # Standard deviation of gaussian \n",
    "    # Input layer (default: 39 -> 128)\n",
    "    self.layer1 = torch.nn.Linear(3 + 3 * 2 * num_encoding_functions, filter_size)\n",
    "    self.layer1_norm = torch.nn.LayerNorm(filter_size)\n",
    "    # Layer 2 (default: 128 -> 128)\n",
    "    self.layer2 = torch.nn.Linear(filter_size, filter_size)\n",
    "    self.layer2_norm = torch.nn.LayerNorm(filter_size)\n",
    "\n",
    "\n",
    "    # Layer 3 (default: 128 -> 4)\n",
    "    self.layer3 = torch.nn.Linear(filter_size, 4)\n",
    "    # Short hand for torch.nn.functional.relu\n",
    "    self.relu = torch.nn.functional.silu\n",
    "    \n",
    "  def forward(self, x0):\n",
    "    x = self.relu(self.layer1_norm(self.layer1(x0)))\n",
    "    x = self.relu(self.layer2_norm(self.layer2(x)))\n",
    "\n",
    "    x = self.layer3(x)\n",
    "    r2 = (x0[:,:3]**2).sum(axis=1)\n",
    "    x[:,3] += self.l_s*torch.exp(-r2/(2*self.s_s**2))\n",
    "    x = x*(r2[:,None]<r2_max)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUnoBIhyKgx6"
   },
   "source": [
    "## Dataloading utils (tinyNeRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v9SbAqC6Ki9H"
   },
   "outputs": [],
   "source": [
    "# Pytorch-tinyNerf\n",
    "def get_minibatches(inputs: torch.Tensor, chunksize: Optional[int] = 1024 * 8):\n",
    "  r\"\"\"Takes a huge tensor (ray \"bundle\") and splits it into a list of minibatches.\n",
    "  Each element of the list (except possibly the last) has dimension `0` of length\n",
    "  `chunksize`.\n",
    "  \"\"\"\n",
    "  return [inputs[i:i + chunksize] for i in range(0, inputs.shape[0], chunksize)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ierxw2dsL3pU"
   },
   "source": [
    "## Determine device to run on (GPU vs CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uKNiPtnML8i9"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zB3NGalaLlN1"
   },
   "source": [
    "## Load up input images, poses, intrinsics, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1w2QkjCkLc9Z"
   },
   "outputs": [],
   "source": [
    "\n",
    "sc = 1\n",
    "\n",
    "# Original focal length from the tiny-nerf lego dataset\n",
    "focal_length = torch.tensor(138.8889*sc).cuda() \n",
    "\n",
    "# Height and width of each image\n",
    "height, width = (64*sc, 64*sc) #images.shape[1:3]\n",
    "\n",
    "# Near and far clipping thresholds for depth values.\n",
    "near_thresh = 2.\n",
    "far_thresh = 6.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFUaajNpNNgJ"
   },
   "source": [
    "## Train TinyNeRF! (tinyNerf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One iteration of TinyNeRF (forward pass).\n",
    "def run_one_iter_of_tinynerf(height, width, focal_length, tform_cam2world,\n",
    "                                  near_thresh, far_thresh, depth_samples_per_ray,\n",
    "                                  encoding_function, get_minibatches_function, \n",
    "                                 bg_color):\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "  # Get the \"bundle\" of rays through all image pixels.\n",
    "  ray_origins, ray_directions = get_ray_bundle(height, width, focal_length,\n",
    "                                               tform_cam2world)\n",
    "  \n",
    "  # Sample query points along each ray\n",
    "  query_points, depth_values = compute_query_points_from_rays(\n",
    "      ray_origins, ray_directions, near_thresh, far_thresh, depth_samples_per_ray\n",
    "  )\n",
    "\n",
    "  # \"Flatten\" the query points.\n",
    "  flattened_query_points = query_points.reshape((-1, 3))\n",
    "\n",
    "  batches = get_minibatches_function(flattened_query_points, chunksize=chunksize)\n",
    "  predictions = []\n",
    "  for batch in batches:\n",
    "    encoded_batch = encoding_function(batch)\n",
    "    p = model(encoded_batch)\n",
    "    predictions.append(p)\n",
    "  radiance_field_flattened = torch.cat(predictions, dim=0)\n",
    "\n",
    "\n",
    "  # \"Unflatten\" to obtain the radiance field.\n",
    "  unflattened_shape = list(query_points.shape[:-1]) + [4]\n",
    "  radiance_field = torch.reshape(radiance_field_flattened, unflattened_shape)\n",
    "  \n",
    "  # Perform differentiable volume rendering to re-synthesize the RGB image.\n",
    "  rgb_predicted, depth_predicted,  T_predicted = render_volume_density(radiance_field,\n",
    "                                                                       ray_origins, \n",
    "                                                                       depth_values,\n",
    "                                                                       bg_color)\n",
    "\n",
    "  return rgb_predicted, depth_predicted,  T_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale from [-1,1] (SD) to [0,1] (NeRF) \n",
    "def rescale_dd(img):\n",
    "    return torch.clamp((img + 1.0) / 2.0, min=0.0, max=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameters for TinyNeRF training - works on 3090 \n",
    "\"\"\"\n",
    "\n",
    "# Number of functions used in the positional encoding (Be sure to update the \n",
    "# model if this number changes).\n",
    "num_encoding_functions = 6\n",
    "# Specify encoding function.\n",
    "encode = lambda x: positional_encoding(x, num_encoding_functions=num_encoding_functions)\n",
    "# Number of depth samples along each ray.\n",
    "depth_samples_per_ray = 128\n",
    "\n",
    "# Chunksize (Note: this isn't batchsize in the conventional sense. This only\n",
    "# specifies the number of rays to be queried in one go. Backprop still happens\n",
    "# only after all rays from the current \"bundle\" are queried and rendered).\n",
    "chunksize=4096\n",
    "\n",
    "# Optimizer parameters\n",
    "lr = 5e-4\n",
    "num_iters = 5000\n",
    "\n",
    "# Misc parameters\n",
    "display_every = 50  # Number of iters after which stats are displayed\n",
    "save_every = 500\n",
    "\"\"\"\n",
    "Model\n",
    "\"\"\"\n",
    "model = VeryTinyNerfModel(num_encoding_functions=num_encoding_functions)\n",
    "model.to(device)\n",
    "\n",
    "\"\"\"\n",
    "Optimizer\n",
    "\"\"\"\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\"\"\"\n",
    "Train-Eval-Repeat!\n",
    "\"\"\"\n",
    "\n",
    "# Seed RNG, for repeatability\n",
    "seed = 9458\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Lists to log metrics etc.\n",
    "psnrs = []\n",
    "iternums = []\n",
    "# Use mixed precision training - SD model evaluated in fp16,\n",
    "# NeRF optimized in mixed fp16 / fp32\n",
    "scaler = torch.cuda.amp.GradScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cam_view(radius, phi, theta, offset):\n",
    "    trans_v = lambda v: np.array([\n",
    "            [1, 0, 0, v[0]],\n",
    "            [0, 1, 0, v[1]],\n",
    "            [0, 0, 1, v[2]],\n",
    "            [0, 0, 0, 1],\n",
    "        ], dtype=np.float32)\n",
    "    trans_t = lambda t: np.array([\n",
    "            [1, 0, 0, 0],\n",
    "            [0, 1, 0, 0],\n",
    "            [0, 0, 1, t],\n",
    "            [0, 0, 0, 1],\n",
    "        ], dtype=np.float32)\n",
    "    rotation_phi = lambda phi: np.array([\n",
    "            [1, 0, 0, 0],\n",
    "            [0, np.cos(phi), -np.sin(phi), 0],\n",
    "            [0, np.sin(phi), np.cos(phi), 0],\n",
    "            [0, 0, 0, 1],\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    rotation_theta = lambda th: np.array([\n",
    "            [np.cos(th), 0, -np.sin(th), 0],\n",
    "            [0, 1, 0, 0],\n",
    "            [np.sin(th), 0, np.cos(th), 0],\n",
    "            [0, 0, 0, 1],\n",
    "    ], dtype=np.float32)\n",
    "    \n",
    "    cam_to_world = trans_t(radius)\n",
    "    cam_to_world = rotation_phi(phi / 180. * np.pi) @ cam_to_world\n",
    "    cam_to_world = rotation_theta(theta / 180. * np.pi) @ cam_to_world\n",
    "    cam_to_world = trans_v(offset) @ cam_to_world\n",
    "    cam_to_world = np.array([[-1, 0, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0, 0, 0, 1]],\n",
    "                                dtype=np.float32) @ cam_to_world\n",
    "    return cam_to_world\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View-angle dependent prefixes.\n",
    "# Not the same as in original paper (they may have experimented more,\n",
    "# may be different between Imagen and Stable Diffusion).\n",
    "def view_to_prefix(radius, phi, theta, offset):\n",
    "    if phi < -60:\n",
    "        return \"overhead view of a \"\n",
    "    elif phi > 60:\n",
    "        return \"bottom view of a \"\n",
    "    elif 45 < theta < 135:\n",
    "        return \"side view of a \"\n",
    "    elif 135 < theta < 225:\n",
    "        return \"front view of a \"\n",
    "    elif 225 < theta < 315:\n",
    "        return \"side view of a \"\n",
    "    else:\n",
    "        return \"rear view of a \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional / unconditional guidance weights\n",
    "# This is less than in original paper (100). Experiment for best results\n",
    "scale = 50\n",
    "cond = {}\n",
    "# Ideally pick something with a distinct colour that's also radially symmetric\n",
    "prompt = 'yellow bath duck blender render'\n",
    "# Conditioning for different views. Original model blended these.\n",
    "# Note that even prepending to the prompt isn't very strong guidance\n",
    "# For the duck, if there's a beak on one side, the model will complete the face even\n",
    "# with \"rear view\"\n",
    "for p in [\"\", \"overhead view of a \", \"bottom view of a \", \"side view of a \", \"rear view of a \", \"front view of a \"]:\n",
    "    cond[p]= sd_model.get_learned_conditioning([p + prompt ])\n",
    "uc = sd_model.get_learned_conditioning([\"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illumination / camera view direction\n",
    "def illum(phi, theta):\n",
    "    phi = phi/180.0*np.pi\n",
    "    theta = theta/180.0*np.pi\n",
    "    return torch.tensor([np.sin(theta)*np.cos(-phi), np.cos(theta)*np.cos(-phi), np.sin(-phi)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JovhcSy1NIhr",
    "outputId": "604060be-0fdc-4921-842e-bd93aa212a95",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_i = 0\n",
    "for i in range(start_i, start_i+num_iters):\n",
    "\n",
    "  # Select a random DDIM timestep length\n",
    "  idx = torch.randint(len(sampler.ddim_timesteps),(1,))\n",
    "  t = torch.tensor([sampler.ddim_timesteps[idx]]).cuda()\n",
    "\n",
    "  lambda_T = 1e-6 # Transparency loss weight\n",
    "  if i<2500:\n",
    "    lambda_norm = 0 # Normal direction loss weight\n",
    "  elif i<9000:\n",
    "    lambda_norm = 1e-4\n",
    "  else:\n",
    "    lambda_norm = 1e-3\n",
    "\n",
    "  # Randomly select camera angle and position\n",
    "  # Different to paper. Also no rotation of up angle - I suspect that's good to\n",
    "  # add later on for regularization, but not helpful early\n",
    "  theta = np.random.uniform(0, 360)\n",
    "  if np.random.random() < 0.5:\n",
    "      phi = np.random.uniform(-90, 0) \n",
    "  else:\n",
    "      phi = np.random.uniform(-50, 10)\n",
    "  # Vary view distance. This perhaps should be a wider range\n",
    "  radius = np.random.uniform(3, 4)\n",
    "  # Jitter camera position\n",
    "  offset = np.random.uniform(-0.05, 0.05, [3])\n",
    "\n",
    "  # Illuminate from an angle near the camera direction. Probably should also\n",
    "  # illuminate from above sometimes. Would like it if the lower half of the object isn't\n",
    "  # made to be dark. \n",
    "  illum_dir = illum(phi+10*np.random.randn(), theta+10*np.random.randn()).half().cuda()\n",
    "  cam_dir = illum(phi, theta).half().cuda()\n",
    " \n",
    "  if i>4000:\n",
    "    # Random background colours (like Dreamfields) later on\n",
    "    # Not in original paper (but has a NeRF that's better for background)\n",
    "    # Really not sure this is helpful - for one test ended up with more haze\n",
    "    # after switching it on\n",
    "    bg_color = torch.tensor(np.random.uniform(0.0, 0.4, [3])).half().cuda()\n",
    "  else:\n",
    "    # Dull grey background. \n",
    "    bg_color = torch.tensor([0.3, 0.3, 0.3]).half().cuda()\n",
    "  \n",
    "        \n",
    "  target_tform_cam2world = torch.tensor(cam_view(radius, phi, theta, offset)).cuda()\n",
    "\n",
    "  prefix = view_to_prefix(radius, phi, ((theta)%360), offset)\n",
    "  c = cond[prefix]\n",
    "  with torch.cuda.amp.autocast():\n",
    "  # Run one iteration of TinyNeRF and get the rendered RGB image.\n",
    "      rgb_predicted, depth_predicted,  T_predicted \\\n",
    "        = run_one_iter_of_tinynerf(height, width, focal_length,\n",
    "                                           target_tform_cam2world, near_thresh,\n",
    "                                           far_thresh, depth_samples_per_ray,\n",
    "                                           encode, get_minibatches, bg_color)\n",
    "    \n",
    "    \n",
    "      # Rescale image components to (-1, 1)\n",
    "      z0 = 2*(rgb_predicted-0.5)\n",
    "      z = z0.permute(2,0,1).unsqueeze(0)\n",
    "      # Upscale to 512x512 - stable diffusion gives poor results when naively\n",
    "      # downsampled from trained resolution\n",
    "      z = torch.nn.functional.interpolate(z, size=(512, 512), mode='bilinear')\n",
    "      # Embed in SD latent space\n",
    "      x = sd_model.get_first_stage_encoding(sd_model.encode_first_stage(z))\n",
    "      # Generate random noise\n",
    "      eps = torch.randn(x.shape).cuda()\n",
    "      # Use noise level from DDIM schedule. Note that ldm SD alpha == alpha^2 in Dreamfusion paper\n",
    "      alpha_t = sampler.ddim_alphas[idx].cuda()\n",
    "      # Add noise to x\n",
    "      x_t = alpha_t.sqrt() * x + torch.sqrt(1-alpha_t) * eps\n",
    "    \n",
    "      # Guided diffusion - evaluate with and without conditional prompt\n",
    "      x_in = torch.cat([x_t] * 2)\n",
    "      t_in = torch.cat([t] * 2)\n",
    "      c_in = torch.cat([uc, c])\n",
    "      with torch.no_grad(): # No gradients in SD UNet required\n",
    "          # Apply UNet to estimate noise vector\n",
    "          e_t_uncond, e_t = sd_model.apply_model(x_in, t_in, c_in).chunk(2)\n",
    "          # Combine conditional and unconditional results according to scale\n",
    "          e_t = e_t_uncond + scale * (e_t - e_t_uncond)\n",
    "          #\n",
    "          d = e_t - eps\n",
    "          # Predict noise-free x using estimated noise\n",
    "          pred_x0 = (x_t - torch.sqrt(1-alpha_t) * e_t) / alpha_t.sqrt()\n",
    "          # Decode 64x64x4 latent into 512x512x3 image space\n",
    "          y = sd_model.decode_first_stage(pred_x0)\n",
    "      # Now apply gradients to embedding.\n",
    "    \n",
    "      # Regularization to try to remove hazyness from NeRF volume\n",
    "      # Original paper used sqrt(lambda_T**2 + 1e-3).sum()\n",
    "      loss_T = lambda_T*T_predicted.sum()\n",
    "      # Normal direction regularization loss - penalize accumulation from backfaces.\n",
    "      # Tries to ensure that the object is dense enough to block transmission from its\n",
    "      # rear. Also smooth surface out\n",
    "      \n",
    "      loss = loss_T \n",
    "      # Unscaled gradients *seem* to work OK, but unsure if need two scalers to make\n",
    "      # work properly\n",
    "      x.backward(gradient=d, retain_graph=True)\n",
    "      \n",
    "  ### AMP loss scaling and update\n",
    "  scaler.scale(loss).backward()\n",
    "  scaler.step(optimizer)\n",
    "  scaler.update()\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  # Display images/plots/stats\n",
    "  if i % display_every == 0:\n",
    "    print(prefix + prompt, loss.detach(), loss_T.detach() )\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.subplot(221)\n",
    "    plt.imshow(rgb_predicted.detach().cpu().float().numpy())\n",
    "    plt.title(f\"Iteration {i}\")\n",
    "    plt.subplot(222)\n",
    "    plt.imshow(rescale_dd(y[0]).detach().cpu().float().numpy().transpose(1,2,0))\n",
    "    plt.title(\"Target\")\n",
    "    plt.subplot(223)\n",
    "    plt.imshow(depth_predicted.detach().cpu().float().numpy())\n",
    "    plt.title(\"Depth\")\n",
    "    \n",
    "    plt.subplot(224)\n",
    "    plt.imshow(T_predicted.detach().cpu().float().numpy())\n",
    "    plt.title(\"Transparency\")\n",
    "\n",
    "    \n",
    "    plt.show()\n",
    "  if (i+1) % save_every == 0:\n",
    "    torch.save({\n",
    "            'epoch': i,\n",
    "            'model1_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scaler_state_dict': scaler.state_dict(),\n",
    "            }, f'checkpoint_{prompt}_{i}.ckpt')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From mipnerf https://github.com/google/mipnerf\n",
    "def generate_spherical_cam_to_world(radius, n_poses=120, d_th=-5, d_phi=-5):\n",
    "    \"\"\"\n",
    "    Generate a 360 degree spherical path for rendering\n",
    "    ref: https://github.com/kwea123/nerf_pl/blob/master/datasets/llff.py\n",
    "    ref: https://github.com/yenchenlin/nerf-pytorch/blob/master/load_blender.py\n",
    "    Create circular poses around z axis.\n",
    "    Inputs:\n",
    "        radius: the (negative) height and the radius of the circle.\n",
    "    Outputs:\n",
    "        spheric_cams: (n_poses, 3, 4) the cam to world transformation matrix of a circular path\n",
    "    \"\"\"\n",
    "\n",
    "    def spheric_pose(theta, phi, radius):\n",
    "        trans_t = lambda t: np.array([\n",
    "            [1, 0, 0, 0],\n",
    "            [0, 1, 0, 0],\n",
    "            [0, 0, 1, t],\n",
    "            [0, 0, 0, 1],\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "        rotation_phi = lambda phi: np.array([\n",
    "            [1, 0, 0, 0],\n",
    "            [0, np.cos(phi), -np.sin(phi), 0],\n",
    "            [0, np.sin(phi), np.cos(phi), 0],\n",
    "            [0, 0, 0, 1],\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "        rotation_theta = lambda th: np.array([\n",
    "            [np.cos(th), 0, -np.sin(th), 0],\n",
    "            [0, 1, 0, 0],\n",
    "            [np.sin(th), 0, np.cos(th), 0],\n",
    "            [0, 0, 0, 1],\n",
    "        ], dtype=np.float32)\n",
    "        cam_to_world = trans_t(radius)\n",
    "        cam_to_world = rotation_phi(phi / 180. * np.pi) @ cam_to_world\n",
    "        cam_to_world = rotation_theta(theta) @ cam_to_world\n",
    "        cam_to_world = np.array([[-1, 0, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0, 0, 0, 1]],\n",
    "                                dtype=np.float32) @ cam_to_world\n",
    "        return cam_to_world\n",
    "\n",
    "    spheric_cams = []\n",
    "    for th in np.linspace(0, 2 * np.pi, n_poses + 1)[:-1]:\n",
    "        spheric_cams += [spheric_pose(th, -30, radius)]\n",
    "    illum_dir = []\n",
    "    for th in np.linspace(0, 2 * np.pi, n_poses + 1)[:-1]:\n",
    "      illum_dir += [illum(30+d_phi, 180+th*180/np.pi+d_th) ]\n",
    "        \n",
    "    return np.stack(spheric_cams, 0), np.stack(illum_dir, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Really simple rendering\n",
    "### Note that volume rendering for training needs to use autograd for\n",
    "### gradient of NeRF field, with retain_graph=True. Very VRAM costly so\n",
    "### needs to be removed for rendering at higher resolutions (sc>2)\n",
    "### This outputs 120 128x128 images at different view angles\n",
    "\n",
    "sc = 2\n",
    "near_thresh=1\n",
    "far_thresh=4.5\n",
    "height = width = 64\n",
    "depth_samples_per_ray = 128\n",
    "chunksize=1024\n",
    "\n",
    "for j in range(120): \n",
    "  target_tform_cam2world = torch.tensor(cam_view(3.5, -30, 360/120*j, np.array([0,0,0]))).to(device) #poses[target_img_idx].to(device)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    rgb_predicted, depth_predicted, _ = run_one_iter_of_tinynerf(height*sc, width*sc, focal_length*sc,\n",
    "                                           target_tform_cam2world, near_thresh,\n",
    "                                           far_thresh, depth_samples_per_ray,\n",
    "                                           encode, get_minibatches, \n",
    "                                           torch.tensor([0.0, 0.0, 0.0]).half().cuda(),\n",
    ")                                    \n",
    "                                \n",
    "    \n",
    "  \n",
    "  print(rgb_predicted.shape, rgb_predicted.min(), rgb_predicted.max())\n",
    "  plt.figure()\n",
    "\n",
    "  plt.imshow(rgb_predicted.detach().cpu().float().numpy())\n",
    "    \n",
    "  im = (rgb_predicted.detach().cpu().float().numpy()*255).astype(np.uint8)\n",
    "  imsave(f'anim_{j:03d}.png', im)\n",
    "  plt.title(f\"View {j:03d}\")\n",
    "  plt.savefig(f\"View_{j:03d}.png\")\n",
    "\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
